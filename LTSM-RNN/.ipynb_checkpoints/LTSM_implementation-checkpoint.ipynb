{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#Understanding torch computational graphs and LTSMs\n",
    "This module serves as an exercise in understanding how computational\n",
    "graphs can be modelled using lua.\n",
    "\n",
    "Given an ltsm rnn, we can think of the algorithm of being comprised of 4 main stages.\n",
    "\n",
    "1. The forget gate: f_t = sigma(Wf(ht_1, x_t) + b_f)S\n",
    "2. The input gate: \n",
    "    i_t = sigma(Wi(ht_1, x_t) + b_i)\n",
    "    ~c_t = tanh(Wc(ht_1, x_t) +b_c)\n",
    "3. Generate new cell state:\n",
    "    c_t = f_t*(c_t-1) + i_t * c_t\n",
    "4. Output decision:\n",
    "    o_t = sigma(Wo(ht_1, x_t) + b_o)\n",
    "    h_t = tanh(c_t)*o_t\n",
    "                \n",
    "The model descbribed below impelements a computational graph using lua tables to make computation more effecient and faster."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#STEP 1: \n",
    "Join the input sequence $x_{t}$, the previous cell state $c_{t}$ and the previous hidden state $h_{t-1}$ into a lua table.\n",
    "![Step 1: Generate input table](img/c1.png)\n",
    "Where the output preactivation vector is then split into the following array:\n",
    "${i, rnn_size, 2*rnn\\_size+i, 3*rnn\\_size+i}$. This is done by the nn graph module functions:\n",
    "\n",
    "nn.Narrow(dim, start, len)(preactivation)\n",
    "eg for the $pre\\_sigmoid\\_chunk$ = nn.Narrow(2, 1, 3*$rnn\\_size$)(preactivations):\n",
    "translation, from the 2nd col of the vector preactivations exctract a vector of length 3*rnn\\_size"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#Step 2:\n",
    "![Step 2: split up components into their respective outputs before passing through a sigmoid layer](img/c2.png)\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#Step 3:\n",
    "![Step 3: Convert the input x into the current cell state](img/c3.png)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "--This module creates the described computational graph\n",
    "require 'nn'\n",
    "require 'nngraph'\n",
    "\n",
    "local LSTM = {}\n",
    "\n",
    "function LSTM.create(input_size, rnn_size)\n",
    "--Definition of some of these functions:\n",
    "--nn.Narrow(dim, start, len) - selects a subvector along \n",
    "--dim dimension having len elements starting from start index\n",
    "    \n",
    "--nn.CMulTable() - outputs the product of tensors in forwarded table\n",
    "--nn.CAddTable() - outputs the sum of tensors in forwarded table\n",
    "  --------------------- input structure ---------------------\n",
    "   \n",
    "  ------------------------------------------------------------  \n",
    "  ----Step 1, as shown in the above computational graphs\n",
    "  ------------------------------------------------------------\n",
    "  local inputs = {}\n",
    "  table.insert(inputs, nn.Identity()())   -- network input\n",
    "  table.insert(inputs, nn.Identity()())   -- c at time t-1\n",
    "  table.insert(inputs, nn.Identity()())   -- h at time t-1\n",
    "  local input = inputs[1]\n",
    "  local prev_c = inputs[2]\n",
    "  local prev_h = inputs[3]\n",
    "\n",
    "  --------------------- preactivations ----------------------\n",
    "  local i2h = nn.Linear(input_size, 4 * rnn_size)(input)   -- input to hidden\n",
    "  local h2h = nn.Linear(rnn_size, 4 * rnn_size)(prev_h)    -- hidden to hidden\n",
    "  local preactivations = nn.CAddTable()({i2h, h2h})        -- i2h + h2h\n",
    "\n",
    "  \n",
    "    \n",
    "  ----Step 2, as shown in the above computational graphs \n",
    "  ------------------ non-linear transforms ------------------\n",
    "  -- gates\n",
    "\n",
    "  local pre_sigmoid_chunk = nn.Narrow(2, 1, 3 * rnn_size)(preactivations)\n",
    "  local all_gates = nn.Sigmoid()(pre_sigmoid_chunk)\n",
    "\n",
    "  -- input\n",
    "  local in_chunk = nn.Narrow(2, 3 * rnn_size + 1, rnn_size)(preactivations)\n",
    "  local in_transform = nn.Tanh()(in_chunk)\n",
    "\n",
    "  ---------------------- gate narrows -----------------------\n",
    "  local in_gate = nn.Narrow(2, 1, rnn_size)(all_gates)\n",
    "  local forget_gate = nn.Narrow(2, rnn_size + 1, rnn_size)(all_gates)\n",
    "  local out_gate = nn.Narrow(2, 2 * rnn_size + 1, rnn_size)(all_gates)\n",
    "  --------------------------------------------------------------\n",
    "    \n",
    "  ---- Step 3, as in comp graph above---------------------------\n",
    "  --------------------- next cell state ---------------------\n",
    "  local c_forget = nn.CMulTable()({forget_gate, prev_c})  -- previous cell state contribution\n",
    "  local c_input = nn.CMulTable()({in_gate, in_transform}) -- input contribution\n",
    "  local next_c = nn.CAddTable()({\n",
    "    c_forget,\n",
    "    c_input\n",
    "  })\n",
    "\n",
    "  -------------------- next hidden state --------------------\n",
    "  local c_transform = nn.Tanh()(next_c)\n",
    "  local next_h = nn.CMulTable()({out_gate, c_transform})\n",
    "\n",
    "  --------------------- output structure --------------------\n",
    "  outputs = {}\n",
    "  table.insert(outputs, next_c)\n",
    "  table.insert(outputs, next_h)\n",
    "\n",
    "  -- packs the graph into a convenient module with standard API (:forward(), :backward())\n",
    "  return nn.gModule(inputs, outputs)\n",
    "end\n",
    "\n",
    "return LSTM"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "iTorch",
   "language": "lua",
   "name": "itorch"
  },
  "language_info": {
   "name": "lua",
   "version": "5.1"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
